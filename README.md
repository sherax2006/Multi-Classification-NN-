# Multi-Classification-NN
This repository demonstrates the implementation of a multi-class neural network from scratch in Python, using NumPy. It is designed for educational purposes to help you understand the core concepts of feedforward and backpropagation for classification tasks.

Features:

01: Dynamic number of output neurons: The number of neurons in the output layer is automatically set based on the number of unique classes in the dataset.
02: Feedforward propagation: Computes the network output using matrix multiplication and wights.
03: Softmax activation function: Used in the output layer for multi-class probability prediction.
04: Cross-entropy loss: Measures the difference between predicted probabilities and true labels.
05: Backpropagation: Computes gradients of weights and biases to update the network.
06: Gradient Descent optimizer: Updates weights and biases iteratively to minimize the loss.
07: Validation: Includes calculation of validation loss and accuracy.

Learning Outcomes:
01: By following this project, you will gain:
02: Clear understanding of feedforward propagation in neural networks.
03: Step-by-step comprehension of backpropagation and gradient calculation.
04: Experience implementing multi-class classification using softmax and cross-entropy.
05: Insight into weight updates using Gradient Descent.
